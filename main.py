# main.py
"""
 FastAPI Fundamentals + LangChain Basics (OFFLINE VERSION)
No OpenAI key required â€“ uses a mock local LLM for demo.

Features:
- FastAPI app with endpoints: /health, /chat, /summarize
- Pydantic models, async endpoints, CRUD demo
- Middleware + CORS
- LangChain: PromptTemplate, Chains (LLMChain), Output Parsers (StructuredOutputParser)
- Basic error handling

Run:
1) pip install fastapi uvicorn langchain pydantic python-dotenv
2) uvicorn main:app --reload
"""

from fastapi import FastAPI, HTTPException, Request, status
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field
from typing import List
import asyncio

from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain.output_parsers import StructuredOutputParser, ResponseSchema
from langchain.llms.base import LLM

# ----------------------------
# App & Middleware
# ----------------------------
app = FastAPI(title="Day 2: FastAPI + LangChain Basics (Offline)", version="1.0.0")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.middleware("http")
async def log_requests(request: Request, call_next):
    try:
        response = await call_next(request)
        return response
    except Exception as exc:
        return JSONResponse(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            content={"detail": f"Unhandled server error: {exc}"},
        )

# ----------------------------
# Pydantic Models
# ----------------------------
class HealthResponse(BaseModel):
    status: str
    service: str

class ChatRequest(BaseModel):
    message: str = Field(..., min_length=1)

class ChatResponse(BaseModel):
    reply: str

class SummarizeRequest(BaseModel):
    text: str = Field(..., min_length=10)
    max_words: int = Field(80, ge=20, le=300)

class SummarizeResponse(BaseModel):
    summary: str

# CRUD demo models
class Note(BaseModel):
    id: int
    title: str
    content: str

class NoteCreate(BaseModel):
    title: str
    content: str

# ----------------------------
# In-memory Store (CRUD Demo)
# ----------------------------
NOTES: List[Note] = []
NEXT_ID = 1

# ----------------------------
# LangChain Setup (Offline Mock LLM)
# ----------------------------
class MockLLM(LLM):
    @property
    def _llm_type(self) -> str:
        return "mock-llm"

    async def _acall(self, prompt: str, stop=None, **kwargs) -> str:
        # Very simple offline behavior for demo purposes
        await asyncio.sleep(0.1)
        if "Summarize" in prompt or "SUMMARY" in prompt:
            # Return a tiny structured summary
            return '{"summary": "This is a concise offline summary generated by the mock LLM."}'
        return "This is an offline mock reply from the LLM. Your message was processed successfully."  # chat reply

# Prompt Template 1: Chat
chat_prompt = PromptTemplate(
    template="""
You are a helpful AI assistant.
User: {user_message}
Assistant:""",
    input_variables=["user_message"],
)

# Prompt Template 2: Summarization
summary_prompt = PromptTemplate(
    template="""
Summarize the following text in at most {max_words} words. Be concise and clear.

TEXT:
{text}

SUMMARY:""",
    input_variables=["text", "max_words"],
)

# Output Parser (Structured)
response_schemas = [ResponseSchema(name="summary", description="A concise summary of the text")]
summary_parser = StructuredOutputParser.from_response_schemas(response_schemas)

# ----------------------------
# Endpoints
# ----------------------------
@app.get("/health", response_model=HealthResponse)
async def health():
    return HealthResponse(status="ok", service="fastapi-langchain-day2-offline")

@app.post("/chat", response_model=ChatResponse)
async def chat(req: ChatRequest):
    try:
        llm = MockLLM()
        chain = LLMChain(llm=llm, prompt=chat_prompt)
        result = await chain.apredict(user_message=req.message)
        return ChatResponse(reply=result.strip())
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Chat failed: {e}")

@app.post("/summarize", response_model=SummarizeResponse)
async def summarize(req: SummarizeRequest):
    try:
        llm = MockLLM()
        format_instructions = summary_parser.get_format_instructions()
        prompt_with_format = PromptTemplate(
            template=summary_prompt.template + "\n\n{format_instructions}",
            input_variables=["text", "max_words", "format_instructions"],
        )
        chain = LLMChain(llm=llm, prompt=prompt_with_format)
        raw = await chain.apredict(text=req.text, max_words=req.max_words, format_instructions=format_instructions)
        parsed = summary_parser.parse(raw)
        return SummarizeResponse(summary=parsed["summary"])
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Summarization failed: {e}")

# ----------------------------
# Basic CRUD (async)
# ----------------------------
@app.post("/notes", response_model=Note, status_code=201)
async def create_note(note: NoteCreate):
    global NEXT_ID
    new_note = Note(id=NEXT_ID, title=note.title, content=note.content)
    NEXT_ID += 1
    NOTES.append(new_note)
    return new_note

@app.get("/notes", response_model=List[Note])
async def list_notes():
    return NOTES

@app.get("/notes/{note_id}", response_model=Note)
async def get_note(note_id: int):
    for n in NOTES:
        if n.id == note_id:
            return n
    raise HTTPException(status_code=404, detail="Note not found")

@app.delete("/notes/{note_id}", status_code=204)
async def delete_note(note_id: int):
    global NOTES
    before = len(NOTES)
    NOTES = [n for n in NOTES if n.id != note_id]
    if len(NOTES) == before:
        raise HTTPException(status_code=404, detail="Note not found")
    return JSONResponse(status_code=204, content=None)

# ----------------------------
# Run locally:
# uvicorn main:app --reload
# ----------------------------
